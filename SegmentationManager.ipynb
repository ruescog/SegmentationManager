{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import torch\n",
    "import json\n",
    "from imutils import paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/.virtualenvs/mmseg/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "%run utils.ipynb\n",
    "%run DatasetManager.ipynb\n",
    "%run TransformManager.ipynb\n",
    "%run ValidationManager.ipynb\n",
    "%run DataLoaderManager.ipynb\n",
    "%run ModelManager.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationManager():\n",
    "    \"\"\"\n",
    "    Manager to use the ModelManager class.\n",
    "    \"\"\"\n",
    "    LIBRARIES = {\n",
    "        \"smp\": ModelManagerSMP,\n",
    "        \"semtorch\": ModelManagerSemtorch,\n",
    "        \"segmentron\": ModelManagerSegmenTron,\n",
    "        \"mmsegmentation\": ModelManagerMMSegmentation\n",
    "    }\n",
    "    \n",
    "    def __init__(self, dataset_manager,\n",
    "                 transform_manager = None,\n",
    "                 validation_manager = None,\n",
    "                 metrics = [DiceMulti()],\n",
    "                 metrics_order = [(1, True)],\n",
    "                 gpu_device = 0\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        Builds the interface.\n",
    "        \n",
    "        Parameters:\n",
    "        dataset_manager (DatasetManager | str, None): the DatasetManager or the root_dir for the data.\n",
    "        transform_manager (TransformManager, TransformManager): the TransfomManager.\n",
    "        validation_manager (ValidationManager, ValidationManagerTrainValTest): the ValidationManager.\n",
    "        metrics (List[function]): list of metrics used to evaluate the models. train_loss and valid_loss are appended to this list in the indexes 0 and 1 respectively.\n",
    "        metrics_order (List[Tuple(int, boolean)], [(1, True)]): set the order of the metrics to determine when a model is better than an other one. valid_loss is used by default (index 1), and this function must be minimized (true).\n",
    "        gpu_device (int): the used gpu identifier.\n",
    "        \n",
    "        Returns:\n",
    "        None.\n",
    "        \"\"\"\n",
    "        self.dataset_manager_ = dataset_manager if type(dataset_manager) != str else SegmentationManager.build_default_dataset(dataset_manager)\n",
    "        self.transform_manager_ = transform_manager if transform_manager else SegmentationManager.build_default_transformation()\n",
    "        self.validation_manager_ = validation_manager if validation_manager else SegmentationManager.build_default_validation()\n",
    "        self.metrics_ = metrics\n",
    "        \n",
    "        # gets the correct metric indexes (+3 because the file has 3 columns before the first metric)\n",
    "        self.metrics_order_ = [(value + 3, must_minimize) for value, must_minimize in metrics_order]\n",
    "        \n",
    "        # best model founded at this moment\n",
    "        self.best_model_ = None\n",
    "        \n",
    "        # fixes the gpu\n",
    "        torch.cuda.set_device(torch.device(f\"cuda:{gpu_device}\"))\n",
    "        self.gpu_device_ = gpu_device\n",
    "    \n",
    "    def get_codes_template():\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        Gets the template for the codes file for the dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        None.\n",
    "        \n",
    "        Returns:\n",
    "        d (dict): the template.\n",
    "        \"\"\"\n",
    "        return DatasetManager.get_codes_template()\n",
    "    \n",
    "    def build_default_dataset(root_dir, img_prefix = \"\", mask_prefix = \"\"):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        Builds a default DatasetManager that works good enougth with the SegmentationManager.\n",
    "        \n",
    "        Parameters:\n",
    "        root_dir (str): the root path to the data. It must contain: \"images\" and \"masks\" directories and \"codes.json\" file. To see the codes.json template, type SegmentationManager.get_codes_template().\n",
    "        img_prefix (str, \"\"): the prefix used in the images.\n",
    "        mask_prefix (str, \"\"): the prefix used in the masks.\n",
    "        \n",
    "        Returns:\n",
    "        dm (DatasetManager): the built DatasetManager.\n",
    "        \"\"\"\n",
    "        return DatasetManager(root_dir, img_prefix = img_prefix, mask_prefix = mask_prefix,\n",
    "                              img_suffix = \"\", mask_suffix = \"\", delete_prefixes = False,\n",
    "                              img_map = None, mask_map = None, check_maps = True,\n",
    "                              check_map_fails = None, convert_masks = True, noise_class = 0)\n",
    "    \n",
    "    def build_default_transformation(transformations = []):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        Builds the default transformations list.\n",
    "        \n",
    "        Parametters:\n",
    "        transformations (list[albumentations]): a list of albumentations. By default: RandomRotate90 (50%), Flip (50%), RandomBrightnessContrast (50%). If you want to use this default configuration and something more, use a list like [\"default\", YourTransformation1, YourTransformation2, ...].\n",
    "        \n",
    "        Returns:\n",
    "        tm (TransformationManager): the built TransformationManager.\n",
    "        \"\"\"\n",
    "        return TransformManager(transformations = transformations)\n",
    "    \n",
    "    def build_default_validation(mode = \"kfold\", random_state = None):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        Builds the default validation mode.\n",
    "        \n",
    "        Parameters:\n",
    "        mode (str): the mode used to validate. \"traintest\" or \"kfold\".\n",
    "        random_state (int): random seed used to shuffle the data.\n",
    "        \n",
    "        Returns:\n",
    "        v (ValidationManager): the built validator.\n",
    "        \"\"\"\n",
    "        if mode == \"traintest\":\n",
    "            warnings.warn(\"Using the method traintest may cause data leakage in combination with other features like callbacks.\", UserWarning)\n",
    "            validator = ValidationManagerTrainValTest(train_size = 0.75, val_size = 0.15, test_size = 0.1,\n",
    "                                                      shuffle = True, random_state = random_state);\n",
    "        elif mode == \"kfold\":\n",
    "            validator = ValidationManagerKFold(train_size = 0.9, test_size = 0.1, n_splits = 5,\n",
    "                                               shuffle = True, random_state = random_state);\n",
    "        \n",
    "        return validator\n",
    "    \n",
    "    @AOP.excepter(AttributeError)\n",
    "    def __get_library__(architecture, backbone, weights = WEIGHTS.NONE, library = None):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        Gets the first library that can builds the model.\n",
    "        \n",
    "        Parameters:\n",
    "        architecture (str): the model's architecture.\n",
    "        backbone (str): the model's backbone.\n",
    "        weights (str): the model's weights.\n",
    "        library (str): the finded library. This parameter is used if the user wants to use a specific library.\n",
    "        \n",
    "        Returns:\n",
    "        library (str, str, str | None, str | None): the library and the configuration that is buildable.\n",
    "        \"\"\"\n",
    "        if type(library) is str and library not in SegmentationManager.LIBRARIES:\n",
    "            raise AttributeError(f\"{library} not in SegmentationManager.LIBRARIES\")\n",
    "\n",
    "        _libraries = {library : SegmentationManager.LIBRARIES[library]} if library else SegmentationManager.LIBRARIES\n",
    "        \n",
    "        for library in _libraries:\n",
    "            value = _libraries[library].is_buildable(architecture, backbone, weights)\n",
    "            if value:\n",
    "                return (library, *value)\n",
    "    \n",
    "    def __start_fastai__(self, batch_size):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        Gets the dataloaders from the data to train a fastai learner.\n",
    "        \n",
    "        Parameters:\n",
    "        batch_size (int): the size of each batch.\n",
    "        \n",
    "        Returns:\n",
    "        dictionary (dict): a dict with each fold and dataloader.\n",
    "        \"\"\"\n",
    "        # from generic to fastai albumentations\n",
    "        transform_manager = TransformManagerFastai.from_transform_manager(self.transform_manager_)\n",
    "\n",
    "        # gets the dataloaders\n",
    "        dls = DataLoaderManager(self.dataset_manager_,\n",
    "                                transforms = transform_manager,\n",
    "                                validation = self.validation_manager_,\n",
    "                                batch_size = batch_size\n",
    "                               )\n",
    "        \n",
    "        # gets the dataloaders\n",
    "        dls = dls.get_dataloaders()\n",
    "        \n",
    "        # if the len of the dls dict is 3, it is a traintest evaluation. We do not want to retraing the entire model twice.\n",
    "        if len(dls) == 3:\n",
    "            dls[\"test\"] = dls[\"f_1\"]\n",
    "            del dls[\"f_1\"]\n",
    "\n",
    "        # gets the validation dls for model validation\n",
    "        val_dls = dls[\"validation\"]\n",
    "        del dls[\"validation\"]\n",
    "        \n",
    "        # returns the dataloaders\n",
    "        return dls, val_dls\n",
    "    \n",
    "    def __save_process__(self, model_name, fold):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        Saves the process of training.\n",
    "        \n",
    "        Parameters:\n",
    "        model_name (str): the model's name.\n",
    "        fold (str): the fold's name.\n",
    "        \n",
    "        Returns:\n",
    "        b (boolean): if the process saved is corresponds to the best model.\n",
    "        \"\"\"\n",
    "        history_file = osp.join(self.dataset_manager_.root_dir_, \"history.csv\")\n",
    "        summary_file = osp.join(self.dataset_manager_.root_dir_, \"summary.csv\")\n",
    "        if osp.isfile(history_file):\n",
    "            # loads the history file\n",
    "            df_new = pd.read_csv(history_file)\n",
    "            \n",
    "            # deletes the time column\n",
    "            df_new = df_new.iloc[:, :-1]\n",
    "            \n",
    "            # creates the model_name and fold columns\n",
    "            df_new[\"model_name\"] = [model_name] * len(df_new)\n",
    "            df_new[\"fold\"] = [fold] * len(df_new)\n",
    "            \n",
    "            # sort the columns of the dataframe model + fold + others\n",
    "            df_new = df_new[[\"model_name\", \"fold\", *df_new.columns[:-2]]]\n",
    "\n",
    "            # loads the summary\n",
    "            if osp.isfile(summary_file):\n",
    "                df = pd.read_csv(summary_file)\n",
    "            else:\n",
    "                df = pd.DataFrame(columns = [\"model_name\", \"fold\", \"epoch\", \"train_loss\", \"valid_loss\", *df_new.columns[5:]])\n",
    "            \n",
    "            df = pd.concat([df, df_new])\n",
    "            df.to_csv(summary_file, index = False)\n",
    "\n",
    "            # deletes the history file\n",
    "            os.remove(history_file)\n",
    "\n",
    "            if fold == \"test\":\n",
    "                # gets all the metrics indexes\n",
    "                metrics_indexes = list(map(lambda t: t[0], self.metrics_order_))\n",
    "                metrics_names = [metric for index, metric in enumerate(df.columns) if index in metrics_indexes]\n",
    "                metrics_sorting = list(map(lambda t: t[1], self.metrics_order_))\n",
    "\n",
    "                # sorts the dataframe by the metrics and orders and get the best model\n",
    "                df_sorted = df.sort_values(metrics_names, ascending = metrics_sorting)\n",
    "                \n",
    "                # gets only the testing folds\n",
    "                df_sorted[df_sorted.fold == \"test\"]\n",
    "                \n",
    "                # determines if the model is the best model\n",
    "                best = model_name if df_sorted.empty else df_sorted[df_sorted.fold == \"test\"].iloc[0,0]\n",
    "\n",
    "                # determines if the model is the best\n",
    "                return model_name == best\n",
    "            else:\n",
    "                return False\n",
    "    \n",
    "    def __fix_validation__(self):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        Fixes the summary.csv for validation results.\n",
    "        \n",
    "        Parameters:\n",
    "        None.\n",
    "        \n",
    "        Returns:\n",
    "        None.\n",
    "        \"\"\"\n",
    "        history_file = osp.join(self.dataset_manager_.root_dir_, \"history.csv\")\n",
    "        if osp.isfile(history_file):\n",
    "            # loads the file\n",
    "            df_new = pd.read_csv(history_file)\n",
    "\n",
    "            # fixes it\n",
    "            index = len(df_new.columns) - 1\n",
    "            while index > 1:\n",
    "                df_new.iloc[:, index] = df_new.iloc[:, index - 1]\n",
    "                index -= 1\n",
    "            \n",
    "            df_new[\"epoch\"] = np.nan\n",
    "            df_new[\"train_loss\"] = np.nan\n",
    "\n",
    "            # saves it\n",
    "            df_new.to_csv(history_file, index = False)\n",
    "            \n",
    "    def __fix_mmsegmentation__(self):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        Converts mmsegmentation log file into a history.csv-like file.\n",
    "        \n",
    "        Parameters:\n",
    "        None.\n",
    "        \n",
    "        Returns:\n",
    "        None.\n",
    "        \"\"\"\n",
    "        with open(osp.join(self.dataset_manager_.root_dir_, \"None.log.json\"), \"r\") as f:\n",
    "            data = [json.loads(row) for row in f.read().split(\"\\n\")[1:-1]]\n",
    "            df = pd.DataFrame(data)\n",
    "            df = pd.DataFrame(df.groupby(\"epoch\").mean().to_records())\n",
    "            df[\"train_loss\"] = None\n",
    "            df[\"epoch\"] -= 1\n",
    "            df = df[[\"epoch\", \"train_loss\", \"loss\", \"mIoU\", \"mAcc\"]]\n",
    "            df.columns = [\"epoch\", \"train_loss\", \"valid_loss\", \"mIoU\", \"mAcc\"]\n",
    "        \n",
    "        os.remove(osp.join(self.dataset_manager_.root_dir_, \"None.log.json\"))\n",
    "        df.to_csv(osp.join(self.dataset_manager_.root_dir_, \"history.csv\"), index = False)\n",
    "            \n",
    "    def __clear_memory__():\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        Deletes all the memory in GPU that is not referenced any more.\n",
    "        \n",
    "        Parameters:\n",
    "        None.\n",
    "        \n",
    "        Returns:\n",
    "        None.\n",
    "        \"\"\"\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    def __build_model__(self, library, name, architecture, backbone, weights, dls, learning_rate):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        Creates the model for a given library.\n",
    "        \n",
    "        Parameters:\n",
    "        library (str): the library.\n",
    "        name (str): the model's name.\n",
    "        architecture (str): the model's architecture.\n",
    "        backbone (str): the model's backbone.\n",
    "        weights (str): the model's weights.\n",
    "        dls (DataLoader | dict): the data distribution used to the train.\n",
    "        learning_rate (float): the learning rate value.\n",
    "        \"\"\"\n",
    "        if library == \"semtorch\":\n",
    "            return ModelManagerSemtorch(name = name,\n",
    "                                        architecture = architecture,\n",
    "                                        backbone = backbone,\n",
    "                                        dls = dls,\n",
    "                                        root_dir = self.dataset_manager_.root_dir_,\n",
    "                                        checkpoints_dir = \"checkpoint\",\n",
    "                                        num_classes = len(self.dataset_manager_.class_names_),\n",
    "                                        loss_func = None,\n",
    "                                        opt_func = fastai.optimizer.Adam,\n",
    "                                        lr = learning_rate,\n",
    "                                        image_size = None,\n",
    "                                        metrics = self.metrics_,\n",
    "                                        moms = (0.95, 0.85, 0.95),\n",
    "                                        cbs = None\n",
    "                                       )\n",
    "        elif library == \"smp\":\n",
    "            return ModelManagerSMP(name = name,\n",
    "                                   architecture = architecture,\n",
    "                                   backbone = backbone,\n",
    "                                   weights = weights,\n",
    "                                   dls = dls,\n",
    "                                   root_dir = self.dataset_manager_.root_dir_,\n",
    "                                   checkpoints_dir = \"checkpoint\",\n",
    "                                   num_classes = len(self.dataset_manager_.class_names_),\n",
    "                                   loss_func = None,\n",
    "                                   opt_func = fastai.optimizer.Adam,\n",
    "                                   lr = learning_rate,\n",
    "                                   metrics = self.metrics_,\n",
    "                                   moms=(0.95, 0.85, 0.95),\n",
    "                                   cbs=None\n",
    "                                  )\n",
    "        elif library == \"segmentron\":\n",
    "            return ModelManagerSegmenTron(name = name,\n",
    "                                          architecture = architecture,\n",
    "                                          backbone = backbone,\n",
    "                                          dls = dls,\n",
    "                                          root_dir = self.dataset_manager_.root_dir_,\n",
    "                                          checkpoints_dir = \"checkpoint\",\n",
    "                                          num_classes = len(self.dataset_manager_.class_names_),\n",
    "                                          loss_func = None,\n",
    "                                          opt_func = fastai.optimizer.Adam,\n",
    "                                          lr = learning_rate,\n",
    "                                          metrics = self.metrics_,\n",
    "                                          moms = (0.95, 0.85, 0.95),\n",
    "                                          cbs = None\n",
    "                                         )\n",
    "        elif library == \"mmsegmentation\":\n",
    "            return ModelManagerMMSegmentation(name = self.dataset_manager_.name_,\n",
    "                                              architecture = architecture,\n",
    "                                              backbone = None,\n",
    "                                              weights = weights,\n",
    "                                              root_dir = self.dataset_manager_.root_dir_,\n",
    "                                              batch_size = dls[\"batch_size\"],\n",
    "                                              num_classes = len(self.dataset_manager_.class_names_),\n",
    "                                              train_pipeline = dls[\"train_pipeline\"],\n",
    "                                              test_pipeline = dls[\"test_pipeline\"],\n",
    "                                              data_split = dls[\"data_split\"],\n",
    "                                              gpu_device = self.gpu_device_\n",
    "                                              )\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    @AOP.logger(\"Training the fold FOLD.\", when = \"before\")\n",
    "    def __train_fold_fastai__(self, name, library, architecture, backbone, weights, fold, dls, val_dls,\n",
    "                       mode = \"fine_tune\", n_epochs = 10, n_freeze_epochs = 2, learning_rate = \"best\"):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        Trains a fold for a model.\n",
    "\n",
    "        Parameters:\n",
    "        name (str): the model's name.\n",
    "        library (str): the model's library\n",
    "        architecture (str): the model's architecture.\n",
    "        backbone (str): the model's backbone.\n",
    "        weights (str): the model's weights.\n",
    "        fold (str): the fold whitch is been training.\n",
    "        dls (DataLoader): the data distribution used to the train.\n",
    "        val_dls (DataLoader): the data used for validation.\n",
    "        mode (str, fit): the type of training. fit, fit_one_cycle or fine_tune.\n",
    "        n_epochs (int, 10): the number of epochs to train.\n",
    "        n_freeze_epochs (int, 2): if the mode of training is fine_tune, the number of freeze epochs.\n",
    "        learning_rate (float): the learning rate value.\n",
    "        \n",
    "        Returns:\n",
    "        lr (float): the value of the learning rate.\n",
    "        \"\"\"\n",
    "        # builds the model\n",
    "        model = self.__build_model__(library, name, architecture, backbone, weights, dls, learning_rate)\n",
    "\n",
    "        if learning_rate == \"best\":\n",
    "            learning_rate = model.lr_\n",
    "\n",
    "        # trains it\n",
    "        if mode == \"fit\":\n",
    "            model.fit(n_epochs = n_epochs)\n",
    "        elif mode == \"fit_one_cycle\":\n",
    "            model.fit_one_cycle(n_epochs = n_epochs)\n",
    "        elif mode == \"fine_tune\":\n",
    "            model.fine_tune(n_epochs = n_epochs, n_freeze_epochs = n_freeze_epochs)\n",
    "\n",
    "        is_best = False\n",
    "        if fold == \"test\":\n",
    "            # evaluate the model\n",
    "            is_best = self.__save_process__(name, fold)\n",
    "            model.validate(val_dls)\n",
    "            self.__fix_validation__()\n",
    "\n",
    "        # saves the process\n",
    "        is_best = self.__save_process__(name, fold) or is_best\n",
    "\n",
    "        # if the model is the best one, saves it\n",
    "        if is_best:\n",
    "            # remove the previous best model\n",
    "            if self.best_model_:\n",
    "                os.remove(osp.join(self.dataset_manager_.root_dir_, \"checkpoint\", self.best_model_))\n",
    "\n",
    "            # saves this one\n",
    "            model.save(name = name)\n",
    "            self.best_model_ = name + \".pth\"\n",
    "\n",
    "        return learning_rate\n",
    "    \n",
    "    @AOP.logger(\"Training the fold FOLD.\", when = \"before\")\n",
    "    def __train_fold_mmsegmentation__(self, name, arch_back, weights, batch_size, train_pipeline, test_pipeline,\n",
    "                                      data_split, fold, mode = \"fine_tune\", n_epochs = 10):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        Trains a fold for a model.\n",
    "\n",
    "        Parameters:\n",
    "        name (str): the model's name.\n",
    "        architecture (str): the model's architecture and backbone.\n",
    "        weights (str): the model's weights.\n",
    "        batch_size (int): the batch size.\n",
    "        train_pipeline (list): the pipeline used for training.\n",
    "        test_pipeline (list): the pipeline used for testing.\n",
    "        data_split (dict): the directory structure used for the training.\n",
    "        fold (str): the fold which is been training.\n",
    "        mode (str, fit): the type of training. fit, fit_one_cycle or fine_tune.\n",
    "        n_epochs (int, 10): the number of epochs to train.\n",
    "        \n",
    "        Returns:\n",
    "        None.\n",
    "        \"\"\"\n",
    "        # builds the model\n",
    "        dls = {\n",
    "            \"batch_size\": batch_size,\n",
    "            \"train_pipeline\": train_pipeline,\n",
    "            \"test_pipeline\": test_pipeline,\n",
    "            \"data_split\": data_split\n",
    "        }\n",
    "        model = self.__build_model__(\"mmsegmentation\", name, arch_back, None, weights, dls, None)\n",
    "\n",
    "        # trains it\n",
    "        if mode == \"fit\":\n",
    "            model.fit(n_epochs = n_epochs)\n",
    "        elif mode == \"fit_one_cycle\":\n",
    "            model.fit_one_cycle(n_epochs = n_epochs)\n",
    "        elif mode == \"fine_tune\":\n",
    "            model.fine_tune(n_epochs = n_epochs, n_freeze_epochs = None)\n",
    "\n",
    "        # saves the process\n",
    "        self.__fix_mmsegmentation__()\n",
    "        self.__save_process__(name, fold)\n",
    "        \n",
    "        # creates the checkpoints dir\n",
    "        checkpoint = osp.join(self.dataset_manager_.root_dir_, \"checkpoint\")\n",
    "        checkpoint_mmsegmentation = osp.join(checkpoint, \"mmsegmentation\")\n",
    "        for directory in [checkpoint, checkpoint_mmsegmentation]:\n",
    "            if not osp.isdir(directory):\n",
    "                os.mkdir(directory)  \n",
    "\n",
    "        # move de trained model to the checkpoints file\n",
    "        models_files = paths.list_files(self.dataset_manager_.root_dir_, validExts = \".pth\")\n",
    "        for file in models_files:\n",
    "            model_name = osp.basename(file)\n",
    "            if model_name == \"latest.pth\":\n",
    "                os.rename(file, osp.join(checkpoint_mmsegmentation, name + \".pth\"))\n",
    "            else:\n",
    "                os.remove(file)\n",
    "                \n",
    "    @AOP.logger(\"Training the model MODEL.\", when = \"before\")\n",
    "    @AOP.excepter(LibraryNotFound, ignore = True)\n",
    "    def __train_model__(self, model, batch_size = 4, mode = \"fine_tune\", n_epochs = 10, n_freeze_epochs = 2):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        Trains a model.\n",
    "        \n",
    "        Parameters:\n",
    "        model (tuple(str, str, str, str, float | str): the metamodel description.\n",
    "        batch_size (int, 4): the number of images showed to the model by batch.\n",
    "        mode (str, fit): the type of training. fit, fit_one_cycle or fine_tune.\n",
    "        n_epochs (int, 10): the number of epochs to train.\n",
    "        n_freeze_epochs (int, 2): if the mode of training is fine_tune, the number of freeze epochs.\n",
    "        \n",
    "        Returns:\n",
    "        name (str): the model's name.\n",
    "        \"\"\"\n",
    "        # gets the params\n",
    "        name, architecture, *hiperparams = model\n",
    "        backbone = BACKBONE.NONE\n",
    "        weights = WEIGHTS.NONE\n",
    "        learning_rate = \"best\"\n",
    "        library = None\n",
    "        if len(hiperparams) >= 1:\n",
    "            backbone = hiperparams[0]\n",
    "        if len(hiperparams) >= 2:\n",
    "            weights = hiperparams[1]\n",
    "        if len(hiperparams) >= 3:\n",
    "            learning_rate = hiperparams[2]\n",
    "        if len(hiperparams) >= 4:\n",
    "            library = hiperparams[3]\n",
    "            print(f\"[LOGGER]: Forced to use {library}.\")\n",
    "                \n",
    "        # gets the library which can builds this model\n",
    "        value = SegmentationManager.__get_library__(architecture, backbone, weights, library = library)\n",
    "        if value:\n",
    "            library, architecture, backbone, weights = value\n",
    "        else:\n",
    "            library = None\n",
    "        \n",
    "        # training for fastai\n",
    "        if library in [\"semtorch\", \"smp\", \"segmentron\"]:\n",
    "            # gets the dataloaders\n",
    "            dls, val_dls = self.__start_fastai__(batch_size = batch_size)\n",
    "\n",
    "            # trains the model\n",
    "            for fold, dls in dls.items():\n",
    "                learning_rate = self.__train_fold_fastai__(name = name, library = library, architecture = architecture,\n",
    "                                                    backbone = backbone, weights = weights, fold = fold, dls = dls,\n",
    "                                                    val_dls = val_dls, mode = mode, n_epochs = n_epochs,\n",
    "                                                    n_freeze_epochs = n_freeze_epochs, learning_rate = learning_rate)\n",
    "                SegmentationManager.__clear_memory__()\n",
    "            \n",
    "        elif library == \"mmsegmentation\":\n",
    "            # gets the splits\n",
    "            if self.dataset_manager_.__class__.__name__ != \"DatasetManagerMMSegmentation\":\n",
    "                self.dataset_manager_ = DatasetManagerMMSegmentation.from_dataset_manager(self.dataset_manager_, name)\n",
    "            else:\n",
    "                self.dataset_manager_.name_ = name\n",
    "            \n",
    "            self.dataset_manager_.build_dataset()\n",
    "            transformations = TransformManagerMMSegmentation.from_transform_manager(self.transform_manager_).get_pipeline()\n",
    "            validation = self.validation_manager_.split(self.dataset_manager_)\n",
    "            val = [\n",
    "                {\"train\": validation[f\"f_{index}\"][\"train\"], \"val\": validation[f\"f_{index}\"][\"val\"], \"test\": validation[\"test\"]}\n",
    "                for index in range(1, len(validation))\n",
    "            ]\n",
    "            \n",
    "            # trains the model\n",
    "            for fold, data_split in enumerate(val):\n",
    "                self.__train_fold_mmsegmentation__(name = name, arch_back = architecture, weights = weights,\n",
    "                                                   batch_size = batch_size, train_pipeline = transformations[0],\n",
    "                                                   test_pipeline = transformations[1], data_split = data_split,\n",
    "                                                   fold = f\"f_{fold+1}\", mode = mode, n_epochs = n_epochs)\n",
    "                SegmentationManager.__clear_memory__()\n",
    "                \n",
    "        else:\n",
    "            raise LibraryNotFound(\"The model configuration does not correspond to any library configuration.\")\n",
    "    \n",
    "    @AOP.logger(\"Starting the multiple training.\", when = \"before\")\n",
    "    @AOP.logger(\"Multiple training ended.\")\n",
    "    @AOP.excepter(TypeError)\n",
    "    def multiple_train(self, models, batch_size = 4,\n",
    "                       mode = \"fine_tune\", n_epochs = 10, n_freeze_epochs = 2):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        Trains multiple models.\n",
    "        \n",
    "        Parameters:\n",
    "        models (list[(str, str, str, str, str | float)]): a list with all the metadata of the models to train.\n",
    "        batch_size (int, 4): the number of images showed to the model by batch.\n",
    "        mode (str, fit): the type of training. fit, fit_one_cycle or fine_tune.\n",
    "        n_epochs (int, 10): the number of epochs to train.\n",
    "        learning_rate (float | str | slice, \"best\"): the learning rate used in training.\n",
    "        n_freeze_epochs (int, 2): if the mode of training is fine_tune, the number of freeze epochs.\n",
    "        moms (Tuple[float]): the momentums used in fit_one_cycle training.\n",
    "\n",
    "        Returns:\n",
    "        None.\n",
    "        \"\"\"\n",
    "        names = set(map(lambda t: t[0], models))\n",
    "        if len(names) != len(models):\n",
    "            raise TypeError(\"The models names must be unique.\")\n",
    "            \n",
    "        for model in models:\n",
    "            self.__train_model__(model = model, batch_size = batch_size,\n",
    "                                 mode = mode, n_epochs = n_epochs, n_freeze_epochs = n_freeze_epochs)\n",
    "            SegmentationManager.__clear_memory__()\n",
    "        \n",
    "        \n",
    "    def summary(self, models = \"all\", folds = \"test\", epoch = \"test\", limit = None, **metrics):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        Filters the summary log to shows the results.\n",
    "        \n",
    "        Parameters:\n",
    "        models (str | list[str]): the models to show. If there is only one can be the string (it will check if the str is in the model name). If it is a list, all the names in the list will be checked.\n",
    "        folds (str | list[str]): the folds to show. Same as the models param.\n",
    "        epoch (int): the max. epoch to show. Shows all the epochs before this limit. If -1, shows all the epochs; if \"test\", shows only the last one.\n",
    "        limit (int): the amount of rows to be shown. All by default.\n",
    "        metrics (metric_name = str | boolean; ...): the type of order for this metric. \"asc\" or \"desc\". If it is a boolean, if the data must be ascending ordered by this metric. The order in the list is important to the sort. You can add \"model_name\", \"folds\", \"epoch\", \"train_loss\" and \"valid_loss\" as well.\n",
    "        \n",
    "        Returns:\n",
    "        df (DataFrame): the filtered DataFrame of results.\n",
    "        \"\"\"\n",
    "        summary_file = osp.join(self.dataset_manager_.root_dir_, \"summary.csv\")\n",
    "        if osp.isfile(summary_file):\n",
    "            df = pd.read_csv(summary_file)\n",
    "            \n",
    "            # filter by model\n",
    "            if models != \"all\":\n",
    "                if type(models) is str:\n",
    "                    models = [models]\n",
    "                _dfs = [df[[model in model_name for model_name in df.model_name]] for model in models]\n",
    "                df = pd.concat(_dfs)\n",
    "            \n",
    "            # filter by fold\n",
    "            if folds != \"all\":\n",
    "                if type(folds) is str:\n",
    "                    folds = [folds]\n",
    "                _dfs = [df[[fold_ in fold for fold in df.fold]] for fold_ in folds]\n",
    "                df = pd.concat(_dfs)\n",
    "            \n",
    "            if epoch == \"test\":\n",
    "                df = df[df[\"epoch\"].isna()]\n",
    "            elif epoch != -1:\n",
    "                df = df[df.epoch <= epoch]\n",
    "                    \n",
    "            # order by metrics\n",
    "            if metrics:\n",
    "                metrics_names = list(map(lambda t: t[0], metrics.items()))\n",
    "                metrics_orders = list(map(lambda t: t[1] if type(t[1]) is bool else True if t[1] == \"asc\" else False, metrics.items()))\n",
    "                df.sort_values(metrics_names, ascending = metrics_orders, inplace = True)\n",
    "                \n",
    "            return df.drop_duplicates().reset_index(drop = True).head(limit)\n",
    "    \n",
    "    def plot_train_valid(self, models = \"all\", metric = \"valid_loss\"):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        Shows the training and validation plot.\n",
    "        \n",
    "        Parameters:\n",
    "        models (str | list): the models to add to the plot.\n",
    "        metric (str): the metric to plot.\n",
    "        \n",
    "        Returns:\n",
    "        None.\n",
    "        \"\"\"\n",
    "        summary_file = osp.join(self.dataset_manager_.root_dir_, \"summary.csv\")\n",
    "        if osp.isfile(summary_file):\n",
    "            df = pd.read_csv(summary_file)\n",
    "            \n",
    "            if models != \"all\":\n",
    "                if type(models) is str:\n",
    "                    models = [models]\n",
    "                _dfs = [df[[model in model_name for model_name in df.model_name]] for model in models]\n",
    "                df = pd.concat(_dfs)\n",
    "\n",
    "            df.query(\"fold != 'test'\")\n",
    "            \n",
    "            if not df.empty:\n",
    "                sns.boxplot(x = \"model_name\", y = metric, data = df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
